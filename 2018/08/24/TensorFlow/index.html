<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  

  
  <title>TensorFlow机器学习 | 莫文之家</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="TensorFlow基础Google公司不仅是大数据和云计算的领导者，在机器学习和深度学习领域也有很好的实践和积累，其内部使用的深度学习框架TensorFlow使深度学习爱好者的学习门槛越来越低。TensorFlow作为一个用于机器智能的开源软件库，是目前深度学习的主流框架之一，广泛应用于学术界与工业界。TensorFlow自开源至今，相继推出了分布式版本、服务器框架、可视化Tensorboar">
<meta name="keywords" content="TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow机器学习">
<meta property="og:url" content="https://morvenyang.github.io/2018/08/24/TensorFlow/index.html">
<meta property="og:site_name" content="莫文之家">
<meta property="og:description" content="TensorFlow基础Google公司不仅是大数据和云计算的领导者，在机器学习和深度学习领域也有很好的实践和积累，其内部使用的深度学习框架TensorFlow使深度学习爱好者的学习门槛越来越低。TensorFlow作为一个用于机器智能的开源软件库，是目前深度学习的主流框架之一，广泛应用于学术界与工业界。TensorFlow自开源至今，相继推出了分布式版本、服务器框架、可视化Tensorboar">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://morvenyang.github.io/2018/08/24/TensorFlow/QQ20180830-165702@2x.png">
<meta property="og:image" content="https://morvenyang.github.io/2018/08/24/TensorFlow/QQ20180830-172920@2x.png">
<meta property="og:updated_time" content="2018-08-30T09:29:34.241Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow机器学习">
<meta name="twitter:description" content="TensorFlow基础Google公司不仅是大数据和云计算的领导者，在机器学习和深度学习领域也有很好的实践和积累，其内部使用的深度学习框架TensorFlow使深度学习爱好者的学习门槛越来越低。TensorFlow作为一个用于机器智能的开源软件库，是目前深度学习的主流框架之一，广泛应用于学术界与工业界。TensorFlow自开源至今，相继推出了分布式版本、服务器框架、可视化Tensorboar">
<meta name="twitter:image" content="https://morvenyang.github.io/2018/08/24/TensorFlow/QQ20180830-165702@2x.png">
  
    <link rel="alternate" href="/atom.xml" title="莫文之家" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">莫文之家</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://morvenyang.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-TensorFlow" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/08/24/TensorFlow/" class="article-date">
  <time datetime="2018-08-24T11:17:15.000Z" itemprop="datePublished">2018-08-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      TensorFlow机器学习
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="TensorFlow基础"><a href="#TensorFlow基础" class="headerlink" title="TensorFlow基础"></a>TensorFlow基础</h2><p>Google公司不仅是大数据和云计算的领导者，在机器学习和深度学习领域也有很好的实践和积累，其内部使用的深度学习框架TensorFlow使深度学习爱好者的学习门槛越来越低。TensorFlow作为一个用于机器智能的开源软件库，是目前深度学习的主流框架之一，广泛应用于学术界与工业界。TensorFlow自开源至今，相继推出了分布式版本、服务器框架、可视化Tensorboard以及不胜枚举的模型在该框架下的实现。</p>
<h3 id="TensorFlow的特点"><a href="#TensorFlow的特点" class="headerlink" title="TensorFlow的特点"></a>TensorFlow的特点</h3><p>TensorFlow是Google基于DistBelief研发的第二代人工智能学习系统，其命名来源于自身的运行原理。TensorFlow是一个采用数据流图(Data Flow Graph)、用于数值计算的开源软件库。数据流图用节点(Node)和线(Edge)的有向图来描述数学计算。节点一般用来表示施加的数学操作，但也可以表示数据(Feed In)的起点/输出(Push Out)的终点，或者是读取/写入持久变量(Persistent Variable)的终点。线表示节点之间的输入/输出关系。这些数据线可以传输大小可动态调整的多维数据数组，即张量(Tensor)。</p>
<ul>
<li>高度的灵活性</li>
</ul>
<p>TensorFlow不是一个严格的神经网络库。只要用户可以将计算表示为一个数据流图，就可以使用TensorFlow来构建图，描写驱动计算的内部循环。</p>
<ul>
<li>真正的可移植性</li>
</ul>
<p>TensorFlow既可以在CPU和GPU上运行，又可以运行于台式机、服务器、笔记本电脑等。TensorFlow还可以将训练好的模型作为产品的一部分用于手机App。TensorFlow同样可以将模型作为云端服务运行在自己的服务器上，或者运行于Docker容器。</p>
<ul>
<li>科研与产品无缝对接</li>
</ul>
<p>Google科学家利用TensorFlow尝试新的算法，其产品团队则用TensorFlow来训练和使用计算模型，并直接提供给在线用户。</p>
<ul>
<li>自动求微分</li>
</ul>
<p>基于梯度的机器学习算法受益于TensorFlow自动求微分的能力。用户只需要定义预测模型的结构，将这个结构和目标函数(Objective Function)结合在一起并添加数据，TensorFlow将自动为用户计算相关的微分导数。</p>
<ul>
<li>多语言支持</li>
</ul>
<p>有一个合理的C++使用界面和一个易用的Python使用界面来构建和执行指定的“图”。还支持用户创造自己喜欢的语言界面，比如Go、Java、Lua、JavaScript或者是R语言。</p>
<ul>
<li>性能最优化</li>
</ul>
<p>由于TensorFlow对线程、队列、异步操作等给予最佳支持，使其计算潜能得以有效发挥。TensorFlow可以将硬件的计算潜能全部发挥出来，可充分利用CPU和多CPU。</p>
<h3 id="TensorFlow中的模型"><a href="#TensorFlow中的模型" class="headerlink" title="TensorFlow中的模型"></a>TensorFlow中的模型</h3><p>TensorFlow的三种主要模型：计算模型、数据模型和运行模型。</p>
<ul>
<li>计算模型</li>
</ul>
<p>计算图(Graph)是TensorFlow中一个最基本的概念，是TensorFlow的计算模型。TensorFlow中的所有计算都会被转化为计算图上的节点，可以把计算图看作一种有向图，TensorFlow中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。</p>
<p>在TensorFlow程序中，系统会维护一个默认的计算图，通过tf.get_default_graph()函数可以获取当前默认的计算图，不同的计算图上的张量和运算不会共享。有效地整理TensorFlow中的资源同样也是计算图的重要功能之一。在一个计算图中，可以通过集合(Collection)来管理不同类别的计算资源，比如通过tf.add_to_collection函数可以将资源加入集合中，然后通过tf.get_collection获取集合中的资源。</p>
<ul>
<li>数据模型</li>
</ul>
<p>张量(Tensor)是TensorFlow中一个非常重要的概念，是TensorFlow的数据模型。在TensorFlow程序中，所有数据都可以通过张量的形式来表示。张量的最基本属性是纬度，其中零维张量表示为标量(Scalar)，一维张量表示为向量(Vector)，当维数超过2时，张量就可以理解为n维数组，但在TensorFLow中张量并不是以数的形式实现的，只是对TensorFlow中运算结果的引用。</p>
<p>一个张量中主要保存的是其名字(Name)、维度(Shape)和类型(Dtype)。张量名字作为张量的唯一标志符，描述了张量是如何计算出来的。张量纬度描述的是张量的纬度信息，比如纬度为零，则张量可以表示为标量。</p>
<ul>
<li>运行模型</li>
</ul>
<p>会话(Session)是拥有并管理TensorFlow程序运行时所有资源的概念，是TensorFlow的运行模型。当所有计算完成之后，需要关闭会话来帮助系统回收计算资源，否则就可能产生资源泄漏的问题。</p>
<h3 id="TensorFlow算法的一般流程"><a href="#TensorFlow算法的一般流程" class="headerlink" title="TensorFlow算法的一般流程"></a>TensorFlow算法的一般流程</h3><ul>
<li>导入/生成样本数据集</li>
</ul>
<ul>
<li>转换和归一化数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = tf.nn.batch_norm_with_global_normalization(...)</span><br></pre></td></tr></table></figure>
<ul>
<li>划分样本数据集为训练样本集、测试样本集和验证样本集</li>
</ul>
<ul>
<li>设置机器学习参数（超参数）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = 0.01</span><br><span class="line">batch_size = 100</span><br><span class="line">iterations = 1000</span><br></pre></td></tr></table></figure>
<ul>
<li>初始化变量和占位符</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a_var = tf.constant(42)</span><br><span class="line">x_input = tf.placeholder(tf.float32, [None, input_size])</span><br><span class="line">y_input = tf.placeholder(tf.float32, [None, num_classes])</span><br></pre></td></tr></table></figure>
<ul>
<li>定义模型结构</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = tf.add(tf.mul(x_input, weight_matrix), b_matrix)</span><br></pre></td></tr></table></figure>
<ul>
<li>声明损失函数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(tf.square(y_actual - y_pred))</span><br></pre></td></tr></table></figure>
<ul>
<li>初始化模型和训练模型</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session(graph=graph) as session:</span><br><span class="line">    ...</span><br><span class="line">    session.run(...)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<ul>
<li>评估机器学习模型</li>
</ul>
<ul>
<li>调优超参数</li>
</ul>
<ul>
<li>发布/预测结果</li>
</ul>
<h3 id="声明张量"><a href="#声明张量" class="headerlink" title="声明张量"></a>声明张量</h3><p>TensorFlow的主要数据结构是张量，它用张量来操作计算图。在TensorFlow里可以把变量或者占位符声明为张量。</p>
<ol>
<li><p>固定张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">row_dim = <span class="number">3</span></span><br><span class="line">col_dim = <span class="number">3</span></span><br><span class="line"><span class="comment"># 零张量</span></span><br><span class="line">zero_tsr = tf.zeros([row_dim, col_dim])</span><br><span class="line"><span class="comment"># 单位张量</span></span><br><span class="line">ones_tsr = tf.ones([row_dim, col_dim])</span><br><span class="line"><span class="comment"># 指定维度的常数填充的张量</span></span><br><span class="line">filled_tsr = tf.fill([row_dim, col_dim], <span class="number">42</span>)</span><br><span class="line"><span class="comment"># 用已知常数张量创建一个张量</span></span><br><span class="line">constant_tsr = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>相似形状的张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zero_similar = tf.zeros_like(constant_tsr)</span><br><span class="line">ones_similar = tf.ones_like(constant_tsr)</span><br></pre></td></tr></table></figure>
</li>
<li><p>序列张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear_tsr = tf.linspace(start=<span class="number">0.0</span>, stop=<span class="number">2.0</span>, num=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>随机张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">randunif_tsr = tf.random_uniform([row_dim, col_dim],</span><br><span class="line">                                minval=<span class="number">0</span>, maxval=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">randnorm_tsr = tf.random_normal([row_dim, col_dim],</span><br><span class="line">                               mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">rumcnorm_tsr = tf.truncated_normal([row_dim, col_dim],</span><br><span class="line">                              mean=<span class="number">0.0</span>, stddev=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">shuffled_output = tf.random_shuffle(input_tensor)</span><br><span class="line">cropped_output = tf.random_crop(input_tensor, crop_size)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="使用占位符和变量"><a href="#使用占位符和变量" class="headerlink" title="使用占位符和变量"></a>使用占位符和变量</h3><p>使用TensorFlow计算图的关键工具是占位符和变量，变量是TensorFlow机器学习算法的参数，TensorFlow维护（调整）这些变量的状态来优化机器学习算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">first_var = tf.Variable(tf.zeros([<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line">sess.run(first_var.initializer)</span><br><span class="line">second_var = tf.Variable(tf.zeros_like(first_var))</span><br><span class="line"><span class="comment"># Depends on first_var</span></span><br><span class="line">sess.run(second_var.initializer)</span><br></pre></td></tr></table></figure>
<h3 id="操作（计算）矩阵"><a href="#操作（计算）矩阵" class="headerlink" title="操作（计算）矩阵"></a>操作（计算）矩阵</h3><ol>
<li><p>创建矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">identity_matrix = tf.diag([<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>])</span><br><span class="line">A = tf.truncated_normal([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">B = tf.fill([<span class="number">2</span>, <span class="number">3</span>], <span class="number">5.0</span>)</span><br><span class="line">C = tf.random_uniform([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">D = tf.convert_to_tensor(np.array([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], [<span class="number">-3.</span>, <span class="number">-7.</span>, <span class="number">-1.</span>], [<span class="number">0.</span>, <span class="number">5.</span>, <span class="number">0</span><span class="number">-2.</span>]]))</span><br><span class="line">print(sess.run(identity_matrix))</span><br></pre></td></tr></table></figure>
</li>
<li><p>矩阵的加法和减法<br><code>print(sess.run(A+B))</code><br>[[3.2625878 4.7152586 3.7038145]<br>[4.4590096 5.09959   4.62294  ]]<br><code>print(sess.run(B-B))</code><br>[[0. 0. 0.]<br>[0. 0. 0.]]</p>
</li>
<li><p>矩阵乘法<br><code>print(sess.run(tf.matmul(B, identity_matrix)))</code><br>[[5. 5. 5.]<br>[5. 5. 5.]]</p>
</li>
<li><p>矩阵转置<br><code>print(sess.run(tf.transpose(C)))</code><br>[[0.3840685  0.07532799 0.54872775]<br>[0.4346856  0.8683685  0.05251133]]</p>
</li>
<li><p>逆矩阵<br><code>print(sess.run(tf.matrix_inverse(D)))</code><br>[[-0.5        -0.5        -0.5       ]<br>[ 0.15789474  0.05263158  0.21052632]<br>[ 0.39473684  0.13157895  0.02631579]]</p>
</li>
<li><p>矩阵分解<br><code>print(sess.run(tf.cholesky(identity_matrix)))</code><br>[[1. 0. 0.]<br>[0. 1. 0.]<br>[0. 0. 1.]]</p>
</li>
<li><p>矩阵的特征值和特征向量<br><code>print(sess.run(tf.self_adjoint_eig(D)))</code><br>(array([-10.65907521,  -0.22750691,   2.88658212]), array([[ 0.21749542,  0.63250104, -0.74339638],</p>
<pre><code>[ 0.84526515,  0.2587998 ,  0.46749277],
[-0.4880805 ,  0.73004459,  0.47834331]]))
</code></pre></li>
</ol>
<h3 id="实现激励函数"><a href="#实现激励函数" class="headerlink" title="实现激励函数"></a>实现激励函数</h3><p>激励函数是使用所有神经网络算法的必备“神器”。激励函数的目的是为了调节权重和误差。在TensorFlow中，激励函数是作用在张量上的非线性操作。</p>
<ol>
<li><p>整流线性单元(Rectifier linear unit, ReLU)是神经网络最常用的非线性函数。<br><code>print(sess.run(tf.nn.relu([-3., 3., 10.])))</code></p>
</li>
<li><p>ReLU6<br><code>print(sess.run(tf.nn.relu6([-3., 3., 10.])))</code></p>
</li>
<li><p>sigmoid函数是最常用的连续、平滑的激励函数。表达式：$\frac{1}{1+e^{-x}}$<br><code>print(sess.run(tf.nn.sigmoid([-1., 0., 1.])))</code></p>
</li>
<li><p>双曲正切函数(hyper tangent, tanh)，表达式：<script type="math/tex">\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><br><code>print(sess.run(tf.nn.tanh([-1., 0., 1.])))</code></p>
</li>
<li><p>softsign函数，表达式： <script type="math/tex">\frac{x}{(abs(x)+1)}</script><br><code>print(sess.run(tf.nn.softsign([-1., 0., 1.])))</code></p>
</li>
<li><p>softplus激励函数是ReLU激励函数的平滑版，表达式为：$log(e^x+1)$<br><code>print(sess.run(tf.nn.softplus([-1., 0., 1.])))</code></p>
</li>
<li><p>ELU激励函数(Exponential Linear Unit, ELU)<br><code>print(sess.run(tf.nn.elu([-1., 0., 1.])))</code></p>
</li>
</ol>
<h3 id="读取数据源"><a href="#读取数据源" class="headerlink" title="读取数据源"></a>读取数据源</h3><ol>
<li><p>鸢尾花卉数据集(Iris data)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">print(len(iris.data))</span><br></pre></td></tr></table></figure>
</li>
<li><p>出生体重数据(Birth weight data)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">birthdata_url = <span class="string">'https://www.umass.edu/statdata/statdata/data/lowbwt.dat'</span></span><br><span class="line">birth_file = requests.get(birthdata_url)</span><br><span class="line">birth_data = birth_file.text.split(<span class="string">'\r\n'</span>)[<span class="number">5</span>:]</span><br><span class="line">birth_header = [x <span class="keyword">for</span> x <span class="keyword">in</span> birth_data[<span class="number">0</span>].split(<span class="string">''</span>) <span class="keyword">if</span> len(x)&gt;=<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>波士顿房价数据(Boston Housing data)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">housing_url = <span class="string">'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'</span></span><br><span class="line">housing_header = [<span class="string">'CRIM'</span>, <span class="string">'ZN'</span>, <span class="string">'INDUS'</span>, <span class="string">'CHAS'</span>, <span class="string">'NOX'</span>, <span class="string">'RM'</span>, </span><br><span class="line">                  <span class="string">'AGE'</span>, <span class="string">'DIS'</span>, <span class="string">'RAD'</span>, <span class="string">'TAX'</span>, <span class="string">'PTRATIO'</span>, <span class="string">'B'</span>,</span><br><span class="line">                 <span class="string">'LSTAT'</span>, <span class="string">'MEDV0'</span>]</span><br><span class="line">housing_file = requests.get(housing_url)</span><br><span class="line"></span><br><span class="line">housing_data = [[float(x) <span class="keyword">for</span> x <span class="keyword">in</span> y.split(<span class="string">' '</span>) <span class="keyword">if</span> len(x)&gt;=<span class="number">1</span>] <span class="keyword">for</span> y <span class="keyword">in</span> housing_file.text.split(<span class="string">'\n'</span>) <span class="keyword">if</span> len(y)&gt;=<span class="number">1</span>]</span><br><span class="line">print(len(housing_data))</span><br></pre></td></tr></table></figure>
</li>
<li><p>MNIST手写体字库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">print(len(mnist.train.images))</span><br></pre></td></tr></table></figure>
</li>
<li><p>垃圾短信文本数据集</p>
</li>
</ol>
<h3 id="TensorFlow实现损失函数"><a href="#TensorFlow实现损失函数" class="headerlink" title="TensorFlow实现损失函数"></a>TensorFlow实现损失函数</h3><p>损失函数(loss function)对机器学习来讲是非常重要的。它们度量模型输出值与目标值(target)间的差值。</p>
<ol>
<li>L2正则损失函数（欧拉损失函数）<br>是预测值与目标值差值的平方和，在目标值附近有更好的曲度，机器学习利用这点收敛，并且离目标越近收敛越慢。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_vals = tf.linspace(<span class="number">-1.</span>, <span class="number">1.</span>, <span class="number">500</span>)</span><br><span class="line">target = tf.constant(<span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">l2_y_vals = tf.square(target - x_vals)</span><br><span class="line">l2_y_out = sess.run(l2_y_vals)</span><br></pre></td></tr></table></figure>
<ol>
<li>L1正则损失函数（绝对值损失函数）<br>对差值求绝对值，在目标值附近不平滑，导致算法不能很好地收敛。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">l1_y_vals = tf.abs(target - x_vals)</span><br><span class="line">l1_y_out = sess.run(l1_y_vals)</span><br></pre></td></tr></table></figure>
<ol>
<li><p>Pseudo-Huber损失函数<br>是Huber损失函数的连续、平滑估计，试图利用L1和L2正则削减极值处的陡峭，使得目标值附近连续。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">delta1 = tf.constant(<span class="number">0.25</span>)</span><br><span class="line">phuber1_y_vals = tf.multiply(tf.square(delta1), tf.sqrt(<span class="number">1.</span> + </span><br><span class="line">                             tf.square((target - x_vals) / delta1)) - <span class="number">1.</span>)</span><br><span class="line">phuber1_y_out = sess.run(phuber1_y_vals)</span><br><span class="line">delta2 = tf.constant(<span class="number">5.</span>)</span><br><span class="line">phuber2_y_vals = tf.multiply(tf.square(delta2), tf.sqrt(<span class="number">1.</span> +</span><br><span class="line">                             tf.square((target - x_vals) / delta2)) - <span class="number">1.</span>)</span><br><span class="line">phuber2_y_out = sess.run(phuber2_y_vals)</span><br></pre></td></tr></table></figure>
</li>
<li><p>分类损失函数<br>用来评估预测分类结果。</p>
</li>
<li><p>Hinge损失函数<br>主要用来评估支持向量机算法，有时也用来评估神经网络算法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x_vals = tf.linspace(<span class="number">-3.</span>, <span class="number">5.</span>, <span class="number">500</span>)</span><br><span class="line">target = tf.constant(<span class="number">1.</span>)</span><br><span class="line">targets = tf.fill([<span class="number">500</span>,], <span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line">hinge_y_vals = tf.maximum(<span class="number">0.</span>, <span class="number">1.</span> - tf.multiply(target, x_vals))</span><br><span class="line">hinge_y_out = sess.run(hinge_y_vals)</span><br><span class="line">print(hinge_y_out)</span><br></pre></td></tr></table></figure>
</li>
<li><p>两类交叉熵损失函数(Cross-entropy loss)<br>有时也作为逻辑损失函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xentropy_y_vals = - tf.multiply(target, tf.log(x_vals)) - tf.multiply((<span class="number">1.</span> - target), </span><br><span class="line">                                                                      tf.log(<span class="number">1.</span> - x_vals))</span><br><span class="line">xentropy_y_out = sess.run(xentropy_y_vals)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Sigmoid交叉熵损失函数(Sigmoid cross entropy loss)<br>先把x_vals值通过sigmoid函数转换，再计算交叉熵损失。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xentropy_sigmoid_y_vals = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_vals, labels=targets)</span><br><span class="line">xentropy_sigmoid_y_out = sess.run(xentropy_sigmoid_y_vals)</span><br></pre></td></tr></table></figure>
</li>
<li><p>加权交叉熵损失函数(Weighted corss entropy loss)<br>Sigmoid交叉熵损失函数的加权，对正目标加权。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">weight = tf.constant(<span class="number">0.5</span>)</span><br><span class="line">xentropy_weighted_y_vals = tf.nn.weighted_cross_entropy_with_logits(logits=x_vals, </span><br><span class="line">                                                                    targets=targets, pos_weight=weight)</span><br><span class="line">xentropy_weighted_y_out = sess.run(xentropy_weighted_y_vals)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Softmax交叉熵损失函数(Softmax corss-entropy loss)<br>作用于非归一化的输出结果，只针对单个目标分类的计算损失。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">unscaled_logits = tf.constant([[<span class="number">1.</span>, <span class="number">-3.</span>, <span class="number">10.</span>]])</span><br><span class="line">target_dist = tf.constant([[<span class="number">0.1</span>, <span class="number">0.02</span>, <span class="number">0.88</span>]])</span><br><span class="line">softmax_xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=unscaled_logits, labels=target_dist)</span><br><span class="line">print(sess.run(softmax_xentropy))</span><br></pre></td></tr></table></figure>
</li>
<li><p>稀疏Softmax交叉熵损失函数(Sparse softmax cross-entropy loss)<br>把目标分类为true的转化成index，而Softmax交叉熵损失函数将目标转成概率分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sparse_target_dist = tf.constant([<span class="number">2</span>])</span><br><span class="line">sparse_xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=unscaled_logits, labels=sparse_target_dist)</span><br><span class="line">print(sess.run(sparse_xentropy))</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>用matplotlib绘制回归算法的损失函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x_array = sess.run(x_vals)</span><br><span class="line">plt.plot(x_array, l2_y_out, <span class="string">'b-'</span>, label=<span class="string">'L2 Loss'</span>)</span><br><span class="line">plt.plot(x_array, l1_y_out, <span class="string">'r--'</span>, label=<span class="string">'L1 Loss'</span>)</span><br><span class="line">plt.plot(x_array, phuber1_y_out, <span class="string">'k-'</span>, label=<span class="string">'P-Huber Loss (0.25)'</span>)</span><br><span class="line">plt.plot(x_array, phuber2_y_out, <span class="string">'g:'</span>, label=<span class="string">'P-Huber Loss (5.0)'</span>)</span><br><span class="line">plt.ylim(<span class="number">-0.2</span>, <span class="number">0.4</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>, prop=&#123;<span class="string">'size'</span>: <span class="number">11</span>&#125;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<div align="center">
    <img src="/2018/08/24/TensorFlow/QQ20180830-165702@2x.png" width="400">
</div>

<p>用matplotlib绘制各种分类算法损失函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x_array = sess.run(x_vals)</span><br><span class="line">plt.plot(x_array, hinge_y_out, <span class="string">'b-'</span>, label=<span class="string">'Hinge Loss'</span>)</span><br><span class="line">plt.plot(x_array, xentropy_y_out, <span class="string">'r--'</span>, label=<span class="string">'Cross Entropy Loss'</span>)</span><br><span class="line">plt.plot(x_array, xentropy_sigmoid_y_out, <span class="string">'k-.'</span>, label=<span class="string">'Cross Entropy Sigmoid Loss'</span>)</span><br><span class="line">plt.plot(x_array, xentropy_weighted_y_out, <span class="string">'g:'</span>, label=<span class="string">'Weighted Cross Entropy Loss(x0.5)'</span>)</span><br><span class="line">plt.ylim(<span class="number">-1.5</span>, <span class="number">3</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>, prop=&#123;<span class="string">'size'</span>: <span class="number">11</span>&#125;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<div align="center">
    <img src="/2018/08/24/TensorFlow/QQ20180830-172920@2x.png" width="400">
</div>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://morvenyang.github.io/2018/08/24/TensorFlow/" data-id="ck0riazxm00001qtgcph8sbu3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/08/25/动手学深度学习第一课：从上手到多类分类/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          《动手学深度学习》笔记
        
      </div>
    </a>
  
  
    <a href="/2018/08/19/编写高质量Objective-C/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">《编写高质量Objective-C》笔记</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C语言/">C语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Objective-C/">Objective-C</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markDownd-数学公式/">markDownd 数学公式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/区块链/">区块链</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/大数据/">大数据</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数学/">数学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/软件架构/">软件架构</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/C语言/" style="font-size: 10px;">C语言</a> <a href="/tags/Objective-C/" style="font-size: 10px;">Objective-C</a> <a href="/tags/TensorFlow/" style="font-size: 10px;">TensorFlow</a> <a href="/tags/markDownd-数学公式/" style="font-size: 10px;">markDownd 数学公式</a> <a href="/tags/区块链/" style="font-size: 10px;">区块链</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a> <a href="/tags/数学/" style="font-size: 10px;">数学</a> <a href="/tags/软件架构/" style="font-size: 10px;">软件架构</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/10/04/编写可读代码的艺术/">编写可读代码的艺术</a>
          </li>
        
          <li>
            <a href="/2018/10/02/常用算法学习/">常用算法学习</a>
          </li>
        
          <li>
            <a href="/2018/09/09/结构思考力/">结构思考力</a>
          </li>
        
          <li>
            <a href="/2018/09/08/软件架构/">软件架构原理</a>
          </li>
        
          <li>
            <a href="/2018/09/02/大数据笔记/">大数据笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 morven<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>





  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>